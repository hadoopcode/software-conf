<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true&amp;useSSL=false</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
        <description>username to use against metastore database</description>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456sql</value>
        <description>password to use against metastore database</description>
    </property>

    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.security.authorization.createtable.owner.grants</name>
        <value>ALL</value>
    </property>

    <property>
        <name>hive.metastore.authorization.storage.checks</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>192.168.79.200</value>
    </property>
    
    <property>
  	<name>hive.users.in.admin.role</name>
  	<value>bigdata</value>
    </property>


    <!--============================================-->
    <property>
        <name>hive.execution.engine</name>
        <value>spark</value>
    </property>

    <property>
        <name>spark.yarn.jars</name>
        <value>hdfs://pseudo:9000/spark-jars/*</value>
    </property>


    <property>
        <name>hive.enable.spark.execution.engine</name>
        <value>true</value>
    </property>

    <property>
        <name>spark.home</name>
        <value>/opt/module/spark-hadoop2-without-hive</value>
    </property>

    <property>
        <name>spark.master</name>
        <value>yarn-cluster</value>
    </property>

    <property>
        <name>spark.enentLog.enabled</name>
        <value>true</value>
    </property>

    <property>
        <name>spark.enentLog.dir</name>
        <value>hdfs://pseudo:9000/spark-logs</value>
    </property>

    <property>
        <name>spark.serializer</name>
        <value>org.apache.spark.serializer.KryoSerializer</value>
    </property>

    <property>
        <name>spark.executor.memeory</name>
        <value>2g</value>
    </property>

    <property>
        <name>spark.driver.memeory</name>
        <value>2g</value>
    </property>

    <property>
        <name>spark.executor.extraJavaOptions</name>
        <value>-XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</value>
    </property>

</configuration>


